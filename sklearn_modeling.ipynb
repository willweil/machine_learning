{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, log_loss, average_precision_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import pickle\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "np.random.seed(1234) # Scikit uses the numpy random seed state\n",
    "\n",
    "mass = load_boston()\n",
    "\n",
    "predictors = [var.lower() for var in mass.feature_names]\n",
    "X = pd.DataFrame(mass['data'], columns=predictors)\n",
    "y = pd.Series(mass['target']) # medv\n",
    "\n",
    "X.head(), y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "df.hist(column='A', bins=50)\n",
    "plt.show()\n",
    "\n",
    "df.plot(x='X', y='Y', kind='scatter', title='Test', legend=False, logy=False)\n",
    "plt.xlabel('D Group')\n",
    "plt.ylabel('L')\n",
    "plt.xlim([30, 90])\n",
    "plt.ylim([0.0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['vh_id_nbr'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aggregate by groups\n",
    "df2 = df.groupby('group2').agg({'A':np.mean, 'B':np.mean})\n",
    "df2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [i if i!= 'ime' else 'response_var' for i in df.columns]\n",
    "\n",
    "df['A'] = df['A'].apply(lambda x : 999 if x == 'UN' else x).apply(lambda x : str(int2(x)))\n",
    "df['B'] = df['B'].apply(lambda x : 999 if x in [22,79,99] else x).apply(lambda x : str(int2(x)))\n",
    "df['C'] = df['C'].apply(lambda x : str(int2(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in feat:\n",
    "    if df[i].dtype == 'float64':\n",
    "        df[i] = df[i].astype('float32')\n",
    "    elif train_df[i].dtype == 'int64':\n",
    "        df[i] = df[i].astype('int32')\n",
    "    else:\n",
    "        df[i] = df[i].astype('categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change to universal column names\n",
    "df.rename(columns = {'clm_nbr':'idcol'}, inplace=True)\n",
    "df.rename(columns = {'response_var':'labelcol'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, shuffle=True)\n",
    "\n",
    "\n",
    "# Reset the indexes\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Train/Test size: \")\n",
    "print(\"Train X: {} Train y: {} \\nTest X: {} Test y: {}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data for modeling\n",
    "\n",
    "claim_ind = 'Ind_1'\n",
    "feature_names = ['A', 'B', 'S', 'T']\n",
    "feat= [i for i in df.columns if i not in ['split', 'idcol', 'labelcol']]\n",
    "\n",
    "\n",
    "modeling_df_x =  modeling_df[feature_names]\n",
    "modeling_df_y = modeling_df[claim_ind]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(modeling_df_x, modeling_df_y, train_size=0.7, shuffle=True)\n",
    "\n",
    "# Reset the indexes\n",
    "train_x.reset_index(inplace=True, drop=True)\n",
    "train_y.reset_index(inplace=True, drop=True)\n",
    "test_x.reset_index(inplace=True, drop=True)\n",
    "test_y.reset_index(inplace=True, drop=True)\n",
    "print(\"Train/Test size: \")\n",
    "print(\"Train X: {} Train y: {} \\nTest X: {} Test y: {}\".format(train_x.shape, train_y.shape, test_x.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into T/V\n",
    "random.seed(1423)\n",
    "df['split'] =  df['labelcol'].apply(lambda x: 'train' if random.random() < 0.7 else 'valid')\n",
    "\n",
    "md_train_x = df.ix[df['split'] == 'train', :].reset_index(drop=True)\n",
    "md_train_y = md_train_x.ix[:, 'labelcol']\n",
    "md_valid_x = df.ix[df['split'] == 'valid', :].reset_index(drop=True)\n",
    "md_valid_y = md_valid_x.ix[:, 'labelcol']\n",
    "print('md_train_x.shape:', md_train_x.shape)\n",
    "print('md_train_y.shape:', md_train_y.shape)\n",
    "print('md_valid_x.shape:', md_valid_x.shape)\n",
    "print('md_valid_y.shape:', md_valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Model 1. Logistic Regression\n",
    "###########################################\n",
    "\n",
    "lr1 = LogisticRegression(tol=0.0001, C=0.8, penalty='l1', max_iter=1000)\n",
    "\n",
    "# random.seed(123)\n",
    "# without cv\n",
    "if True:\n",
    "    lr1_pred= pd.Series(lr1.fit(train_x, train_y).predict_proba(test_x)[:,1])\n",
    "    top_5_accuracy(test_y, lr1_pred, 'lr1')\n",
    "    print_auc(test_y, lr1_pred, 'lr1', plot=True)\n",
    "\n",
    "def logicreg_imp_feature(logicreg_model, feat, top_n=20):\n",
    "    print(\"Logistic Regression model Top/Bottom {} feature importance: \\n\".format(top_n))\n",
    "    for indx in logicreg_model.coef_.argsort().ravel()[::-1][list(range(0,top_n))+list(range(-top_n,0))]:\n",
    "        word = feat[indx]\n",
    "        coef = logicreg_model.coef_.ravel()[indx]\n",
    "        if coef != 0:\n",
    "            print(\"{0:s}: {1:.2f}\".format(word,np.exp(coef)))\n",
    "\n",
    "            # feature importance\n",
    "lr_feature_importance = logicreg_imp_feature(lr1, feat, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logicreg_feat_imp(logicreg_model, feature_names, top_n=10, plot=False):\n",
    "    '''\n",
    "    Important features in Logistic Regression\n",
    "    '''\n",
    "    if top_n > len(feature_names):\n",
    "        top_n = len(feature_names)\n",
    "    imp_df = pd.DataFrame({\"feat\": feature_names, \"imp\": np.round(logicreg_model.coef_.ravel(),3)})\n",
    "    imp_df_top = imp_df.sort_values(by= [\"imp\"], ascending= False).iloc[:top_n, :]\n",
    "    print(\"LogicReg model top {} feature importance:\".format(top_n))\n",
    "    print(imp_df_top)\n",
    "    if plot:\n",
    "        # bar graph to show feature importance\n",
    "        pos = np.arange(imp_df_top.shape[0]) + 0.5\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.barh(pos, imp_df_top.imp.values[::-1]*100, align='center')\n",
    "        plt.yticks(pos, imp_df_top.feat.values[::-1])\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance in Logistic Regression\")\n",
    "        plt.show()\n",
    "    return imp_df_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "train_score = ols.score(X_train, y_train)\n",
    "test_score = ols.score(X_test, y_test)\n",
    "\n",
    "print(\"R^2 on Train: {:0.2f}%, on Test: {:0.2f}%\".format(train_score*100, test_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_score = mean_squared_error(y_train, ols.predict(X_train))\n",
    "test_score = mean_squared_error(y_test, ols.predict(X_test))\n",
    "\n",
    "print(\"MSE on Train: {}, on Test: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Model 2: XGBoost\n",
    "####################################\n",
    "# with original API\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x.values, label=train_y)\n",
    "dtest = xgb.DMatrix(test_x.values, label=test_y)\n",
    "\n",
    "evallist = [(dtest,'eval'), (dtrain,'train')]\n",
    "\n",
    "\n",
    "if True:\n",
    "    param = {'max_depth': 6, 'eta': 0.01, 'silent': 1, 'objective': 'binary:logistic',  \n",
    "             'eval_metric':'auc', 'subsample':0.6, 'colsample_bytree':1.0} \n",
    "    num_round = 200\n",
    "\n",
    "    xgb0 = xgb.train(params=param, dtrain=dtrain, num_boost_round=num_round,\n",
    "                     evals=evallist, early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "    xgb0_pred = xgb0.predict(dtest, ntree_limit=xgb0.best_iteration)\n",
    "    top_5_accuracy(test_y, xgb0_pred, 'xgb0')\n",
    "    print_auc(test_y, xgb0_pred, 'xgb0', plot=True)\n",
    "    xgb_feature_importance = xgb_feat_imp(xgb0, feature_names, top_n=10, plot=True)\n",
    "    print_precision_recall_curve(test_y, xgb0_pred, 'xgb0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_feat_imp(xgb_model, feature_names, top_n=10, plot=False):\n",
    "    '''\n",
    "    Important features in XGBoost\n",
    "    '''\n",
    "    if top_n > len(feature_names):\n",
    "        top_n = len(feature_names)\n",
    "    imp_df = pd.DataFrame(pd.Series(xgb_model.get_fscore(), name='imp'))\n",
    "    imp_df['feat'] = imp_df.index\n",
    "    imp_df['feat'] = imp_df['feat'].apply(lambda x: feature_names[int(x[1:])])\n",
    "    imp_df.reset_index(drop=True, inplace=True)\n",
    "    imp_df_top = imp_df.sort_values(by='imp', ascending=False).iloc[:top_n, :]\n",
    "    imp_df_top['imp'] = np.round(imp_df_top['imp'] / imp_df['imp'].sum(), 3)\n",
    "    imp_df_top = imp_df_top[['feat', 'imp']]\n",
    "    print('XGBoost model top {} feature importance:'.format(top_n))\n",
    "    print(imp_df_top)\n",
    "    if plot:\n",
    "        # bar graph to show feature importance\n",
    "        pos = np.arange(imp_df_top.shape[0]) + 0.5\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.barh(pos, imp_df_top.imp.values[::-1]*100, align='center')\n",
    "        plt.yticks(pos, imp_df_top.feat.values[::-1])\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance in XGBoost\")\n",
    "        plt.show()\n",
    "    return imp_df_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_xgb(df, response='ime'):\n",
    "    gc.collect()\n",
    "    # response need to be binary\n",
    "    np.random.seed(123456)\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    md_train_x = df[msk].loc[:, df.columns != response].reset_index(drop=True)\n",
    "    md_train_y = df[msk].loc[:, df.columns == response].reset_index(drop=True)\n",
    "    md_valid_x = df[~msk].loc[:, df.columns != response].reset_index(drop=True)\n",
    "    md_valid_y = df[~msk].loc[:, df.columns == response].reset_index(drop=True)\n",
    "    dtrain = xgb.DMatrix(md_train_x.values, label=md_train_y.values, feature_names=md_train_x.columns)\n",
    "    dvalid = xgb.DMatrix(md_valid_x.values, label=md_valid_y.values, feature_names=md_valid_x.columns)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    param = {'max_depth': 6,\n",
    "            'eta': 0.1,\n",
    "            'silent': 1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'colsample_bytree' : 0.7,\n",
    "            'min_child_weight ': 10,\n",
    "            'nthread': 8,\n",
    "            # 'eval_metric': 'auc',\n",
    "            'eval_metric': 'error',\n",
    "            \"eta_decay\": 0.5,\n",
    "            'scale_pos_weight': 5,\n",
    "            }\n",
    "\n",
    "    num_round = 600\n",
    "\n",
    "    evallist = [(dvalid,'eval'), (dtrain,'train')]\n",
    "    xgb_model = xgb.train(params=param, dtrain=dtrain, num_boost_round=num_round, evals=evallist, early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "    xgb_pred = xgb_model.predict(dvalid, ntree_limit=xgb_model.best_iteration)\n",
    "    xgb_pred_train = xgb_model.predict(dtrain, ntree_limit=xgb_model.best_iteration)\n",
    "\n",
    "    # xgb_obj = \"../xgb_obs_\" + str(obs) + \"features_\"+ str(nf) + \".p\"\n",
    "    # pickle.dump(xgb_model, open(xgb_obj, \"wb\"))\n",
    "\n",
    "    result = [xgb_model, top_5_accuracy(md_valid_y, xgb_pred, 'xgb valid') + '\\n' + top_5_accuracy(md_train_y, xgb_pred_train, 'xgb train')]\n",
    "    return result\n",
    "\n",
    "def plot_xgb_imp(xgb_model, path, txt='', fname='abc'):\n",
    "    with PdfPages(path) as pdf:\n",
    "        f, ax = plt.subplots(figsize=(12,50))\n",
    "        importances = xgb_model.get_fscore()\n",
    "        importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
    "        importance_frame.sort_values(by = 'Importance', inplace = True, ascending=False)\n",
    "        importance_frame.to_csv(\"var_importance_\" + fname + \".csv\", index=False)\n",
    "\n",
    "        importance_frame[:150].sort_values(by = 'Importance', ascending=True).plot(kind = 'barh', x = 'Feature', color = 'orange', ax=ax, grid=True)\n",
    "\n",
    "        ax.text(0.95, 0.04, txt,\n",
    "        verticalalignment='bottom', horizontalalignment='right',\n",
    "        transform=ax.transAxes,\n",
    "        color='blue', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with sklearn wrapper\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "bst = XGBRegressor(learning_rate=0.1\n",
    "                    , max_depth=3\n",
    "                    , min_child_weight=20\n",
    "                    , subsample=0.9\n",
    "                    , n_estimators=1000)\n",
    "\n",
    "bst.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "train_score = mean_squared_error(y_train, bst.predict(X_train, ntree_limit=bst.best_iteration))\n",
    "test_score = mean_squared_error(y_test, bst.predict(X_test, ntree_limit=bst.best_iteration))\n",
    "\n",
    "print(\"MSE on Train: {}, on Test: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with sklearn API\n",
    "\n",
    "xgb1 = xgb.XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=600, silent=False,\n",
    "                         objective='binary:logistic', nthread=-1, subsample=0.6, colsample_bytree=0.7)\n",
    "if True:\n",
    "    xgb1_pred = pd.Series(xgb1.fit(md_train_x[feat], md_train_y, early_stopping_rounds=20, eval_metric=\"error\", \n",
    "                                   eval_set=[(md_valid_x[feat], md_valid_y)], \n",
    "                                   verbose=False).predict_proba(md_valid_x[feat])[:,1])\n",
    "    top_5_accuracy(md_valid_y, xgb1_pred, 'xgb1')\n",
    "    print_auc(md_valid_y, xgb1_pred, 'xgb1')\n",
    "\n",
    "if False:\n",
    "    xgb1_train_cv, xgb1_pred_cv, xgb1_cv = cvModel(md_train_x, md_valid_x, target= md_train_y, model= xgb1, feat= feat,\n",
    "                                          idcol= \"idcol\", nfolds= 4, nreps= 2, classify= True)\n",
    "    top_5_accuracy(xgb1_pred_cv.target.values, xgb1_pred_cv.pred.values, 'xgb1_cv_train')\n",
    "    print_auc(xgb1_pred_cv.target.values, xgb1_pred_cv.pred.values, 'xgb1_cv_train')\n",
    "    top_5_accuracy(md_valid_y, xgb1_pred_cv.pred.values, 'xgb1_cv_pred')\n",
    "    print_auc(md_valid_y, xgb1_pred_cv.pred.values, 'xgb1_cv_pred')\n",
    "\n",
    "# xgb1  top 5% accuracy:  0.47004\n",
    "# xgb1  AUC:  0.89939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Model 3. GBM\n",
    "#######################################################\n",
    "\n",
    "\n",
    "if False:\n",
    "    gbm1 = GradientBoostingClassifier(loss = \"deviance\", learning_rate= 0.05, n_estimators= 100,\n",
    "                                      max_depth=1, min_samples_split= 10, min_samples_leaf= 10,\n",
    "                                      subsample= 0.5, max_features= None, verbose= 0)\n",
    "    gbm1_pred = pd.Series(gbm1.fit(train_x, train_y).predict_proba(test_x)[:,1])\n",
    "    top_5_accuracy(test_y, gbm1_pred, 'gbm1')\n",
    "    print_auc(test_y, gbm1_pred, 'gbm1', plot=True)\n",
    "    gbm_feature_importance = gbm_feat_imp(gbm1, feature_names, top_n=10, plot=True)\n",
    "    print_precision_recall_curve(test_y, gbm1_pred, 'gbm1')\n",
    "    \n",
    "\n",
    "if True:\n",
    "    gbm2 = GradientBoostingRegressor(loss = \"huber\", learning_rate= 0.05, n_estimators= 100,\n",
    "                                     max_depth=4, min_samples_split= 10, min_samples_leaf= 10,\n",
    "                                     subsample= 0.5, max_features= None, verbose= 0)\n",
    "    gbm2_pred = pd.Series(gbm2.fit(train_x, train_y).predict(test_x))\n",
    "    mse = mean_squared_error(test_y, gbm2_pred)\n",
    "    print(\"MSE: {0:.5f} from gbm2\".format(mse))\n",
    "    gbm_feature_importance = gbm_feat_imp(gbm2, feature_names, top_n=10, plot=True)\n",
    "\n",
    "# important variables in gbm\n",
    "top_n = 20\n",
    "gbm_feat_imp = pd.DataFrame({\"feat\": feat, \"imp\": np.round(gbm1.feature_importances_,3)})\n",
    "gbm_feat_imp_top = gbm_feat_imp.sort_values(by= [\"imp\"], ascending= False).iloc[:top_n, :]\n",
    "print(gbm_feat_imp_top)\n",
    "# bar graph to show feature importance\n",
    "gbm_feat_imp_bar = gbm_feat_imp.sort_values(by= [\"imp\"], ascending= True).iloc[-top_n:, :]\n",
    "pos = np.arange(gbm_feat_imp_bar.shape[0]) + 0.5\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.barh(pos, gbm_feat_imp_bar.imp.values*100, align='center')\n",
    "plt.yticks(pos, gbm_feat_imp_bar.feat.values)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Variable Importance in Gradient Boosting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gbm_feat_imp(gbm_model, feature_names, top_n=10, plot=False):\n",
    "    '''\n",
    "    Important features in GBM\n",
    "    '''\n",
    "    if top_n > len(feature_names):\n",
    "        top_n = len(feature_names)\n",
    "    imp_df = pd.DataFrame({\"feat\": feature_names, \"imp\": np.round(gbm_model.feature_importances_,3)})\n",
    "    imp_df_top = imp_df.sort_values(by= [\"imp\"], ascending= False).iloc[:top_n, :]\n",
    "    print('GBM model top {} feature importance:'.format(top_n))\n",
    "    print(imp_df_top)\n",
    "    if plot:\n",
    "        # bar graph to show feature importance\n",
    "        pos = np.arange(imp_df_top.shape[0]) + 0.5\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.barh(pos, imp_df_top.imp.values[::-1]*100, align='center')\n",
    "        plt.yticks(pos, imp_df_top.feat.values[::-1])\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance in GBM\")\n",
    "        plt.show()\n",
    "    return imp_df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Model 4. Random Forests\n",
    "#######################################################\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=1000, min_samples_split=10, criterion='entropy', random_state=123, \n",
    "                             n_jobs=-1, verbose=0)\n",
    "\n",
    "if True:\n",
    "    rf1_pred = pd.Series(rf1.fit(train_x, train_y).predict_proba(test_x)[:,1])\n",
    "    top_5_accuracy(test_y, rf1_pred, 'rf1')\n",
    "    print_auc(test_y, rf1_pred, 'rf1', plot=True)\n",
    "    \n",
    "if False:\n",
    "    rf1_train_cv, rf1_pred_cv, rf1_cv = cvModel(md_train_x, md_valid_x, target=md_train_y, model=rf1, feat=feat,\n",
    "                                          idcol=\"idcol\", nfolds=10, nreps=2, classify=True)\n",
    "    top_5_accuracy(md_valid_y, rf1_pred_cv.pred.values, 'rf1_cv')\n",
    "    print_auc(md_valid_y, rf1_pred_cv.pred.values, 'rf1_cv')\n",
    "\n",
    "# feature importance\n",
    "rf_feature_importance = rf_feat_imp(rf1, feature_names, top_n=10, plot=True)\n",
    "\n",
    "\n",
    "# feature importance\n",
    "rf_feature_importance = pd.DataFrame(list(zip(md_train_x[feat], rf1_cv.feature_importances_)))\n",
    "rf_feature_importance.columns = ['feature', 'importance']\n",
    "rf_feature_importance.sort_values(by='importance', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_feat_imp(rf_model, feature_names, top_n=10, plot=False):\n",
    "    '''\n",
    "    Important features in Random Forest\n",
    "    '''\n",
    "    if top_n > len(feature_names):\n",
    "        top_n = len(feature_names)\n",
    "    imp_df = pd.DataFrame({\"feat\": feature_names, \"imp\": np.round(rf_model.feature_importances_,3)})\n",
    "    imp_df_top = imp_df.sort_values(by= [\"imp\"], ascending= False).iloc[:top_n, :]\n",
    "    print('Random Forest top {} feature importance:'.format(top_n))\n",
    "    print(imp_df_top)\n",
    "    if plot:\n",
    "        # bar graph to show feature importance\n",
    "        pos = np.arange(imp_df_top.shape[0]) + 0.5\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.barh(pos, imp_df_top.imp.values[::-1]*100, align='center')\n",
    "        plt.yticks(pos, imp_df_top.feat.values[::-1])\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance in Random Forest\")\n",
    "        plt.show()\n",
    "    return imp_df_top\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=500\n",
    "                           , max_features=\"sqrt\"\n",
    "                           , max_depth=5\n",
    "                           , min_samples_split=30\n",
    "                           , oob_score=True\n",
    "                            )\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_score = mean_squared_error(y_train, rf.predict(X_train))\n",
    "test_score = mean_squared_error(y_test, rf.predict(X_test))\n",
    "\n",
    "print(\"MSE on Train: {}, on Test: {}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Model 5. ERT\n",
    "#######################################################\n",
    "\n",
    "ert1 = ExtraTreesClassifier(n_estimators=1000, min_samples_split=10, criterion='entropy', random_state=1234, \n",
    "                            n_jobs=-1, verbose=0)\n",
    "\n",
    "if True:\n",
    "    ert1_pred = pd.Series(ert1.fit(md_train_x[feat], md_train_y).predict_proba(md_valid_x[feat])[:,1])\n",
    "    top_5_accuracy(md_valid_y, ert1_pred, 'ert1')\n",
    "    print_auc(md_valid_y, ert1_pred, 'ert1')\n",
    "\n",
    "if False:\n",
    "    ert1_train_cv, ert1_pred_cv, ert1_cv = cvModel(md_train_x, md_valid_x, target=md_train_y, model=ert1, feat=feat,\n",
    "                                          idcol=\"idcol\", nfolds=4, nreps=3, classify=True)\n",
    "    top_5_accuracy(md_valid_y, ert1_pred_cv.pred.values, 'ert1_cv')\n",
    "    print_auc(md_valid_y, ert1_pred_cv.pred.values, 'ert1_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Model 6. LDA topic modeling\n",
    "###########################################\n",
    "\n",
    "# LDA topic\n",
    "n_topics = 10\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,  learning_method='online',\n",
    "                                learning_offset=50., random_state=123, n_jobs=6, verbose=0)\n",
    "lda.fit(tf[feat])\n",
    "#key words\n",
    "# n_top_words = 20\n",
    "# LDA_top_words(lda, tf_vectorizer, n_top_words)\n",
    "\n",
    "# logistic regression on topics\n",
    "lda_train_x = normalize(lda.transform(md_train_x[feat]), norm='l1', axis=1)\n",
    "lda_valid_x = normalize(lda.transform(md_valid_x[feat]), norm='l1', axis=1)\n",
    "lda_topic_lr = LogisticRegression(C=0.5, penalty='l1')\n",
    "lda_topic_lr_pred = pd.Series(lda_topic_lr.fit(lda_train_x, md_train_y).predict_proba(lda_valid_x)[:,1])\n",
    "\n",
    "print(\"{} non-zero features\".format(np.sum(lda_topic_lr.coef_!=0)))\n",
    "# score to get top 5% accuracy and AUC\n",
    "top_5_accuracy(md_valid_y, lda_topic_lr_pred, 'lda_topic_lr')\n",
    "print_auc(md_valid_y, lda_topic_lr_pred, 'lda_topic_lr')\n",
    "\n",
    "# topic importance\n",
    "print(\"topic importance:\")\n",
    "for indx in lda_topic_lr.coef_.argsort().ravel()[::-1]:\n",
    "    coef = lda_topic_lr.coef_.ravel()[indx]\n",
    "    if coef != 0:\n",
    "        print(\"Topic {0:n}: {1:.2f}\".format(indx,np.exp(coef)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Model 7. SGD\n",
    "#######################################################\n",
    "\n",
    "sgd1 = SGDClassifier(loss=\"modified_huber\", penalty=\"l1\", n_iter=100, random_state=1234, n_jobs=6)\n",
    "sgd1.fit(md_train_x[feat], md_train_y)\n",
    "\n",
    "sgd1_pred = pd.Series(sgd1.predict_proba(md_valid_x[feat])[:,1])\n",
    "\n",
    "# score to get top 5% accuracy\n",
    "top_5_accuracy(md_valid_y, sgd1_pred, 'sgd1')\n",
    "print_auc(md_valid_y, sgd1_pred, 'sgd1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kf(X=X_train):\n",
    "    kf = KFold(n_splits=5, random_state=1234)\n",
    "    return kf.split(X) # we can plug this generator into later functions to run CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize a Random Forest\n",
    "rf_cv = RandomForestRegressor()\n",
    "\n",
    "# Set up the list of parameters to grid over\n",
    "param_grid = {\n",
    "        'n_estimators': [100, 500, 750, 1000],\n",
    "        'max_features': [\"sqrt\", 0.33],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [2, 15, 30, 50]\n",
    "}\n",
    "\n",
    "# Using the empty RF as a base, pass the parameters into a GridSearchCV\n",
    "cv_grid = GridSearchCV(rf_cv, param_grid, cv=kf(), n_jobs=12) # n_folds=12 adds multithreading with 12 cores\n",
    "cv_grid.fit(X_train, y_train)\n",
    "\n",
    "# GridSearchCV .predict() uses whichever parameters gave the best mean OOF score\n",
    "train_score = mean_squared_error(y_train, cv_grid.predict(X_train))\n",
    "test_score = mean_squared_error(y_test, cv_grid.predict(X_test))\n",
    "\n",
    "print(\"MSE on Train: {}, on Test: {}\".format(train_score, test_score))\n",
    "\n",
    "print(\"Best Parameters: {}\".format(cv_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random search of weights for ensembling\n",
    "\n",
    "models = pd.DataFrame([lr1_pred, xgb1_pred, gbm1_pred, rf1_pred, ert1_pred])\n",
    "# models = pd.DataFrame([lr1_pred_cv.pred, xgb1_pred_cv.pred, gbm1_pred_cv.pred, rf1_pred_cv.pred, ert1_pred_cv.pred])\n",
    "models = models.T\n",
    "models.columns = ['lr1_pred', 'xgb1_pred', 'gbm1_pred', 'rf1_pred', 'ert1_pred']\n",
    "weights, _ = random_search_ensemble_weights(models, target=md_valid_y, n_random_runs=100000, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def random_search_ensemble_weights(models, target, n_random_runs=100, random_state=123456):\n",
    "    n_models = models.shape[1]\n",
    "    print('Number of models to ensemble: {}'.format(n_models))\n",
    "    random.seed(random_state)\n",
    "    accuracy_cup = []\n",
    "    w = []\n",
    "    for i in range(n_random_runs):\n",
    "        w = [random.uniform(0.0, 1.0) for j in range(n_models)]\n",
    "        w = np.array(w) / sum(w)\n",
    "        models['pred_combined'] = models.iloc[:, 0] * w[0]\n",
    "        for m in range(1, n_models):\n",
    "            models['pred_combined'] = models['pred_combined'] + models.iloc[:, m] * w[m]\n",
    "\n",
    "        actual_pred = pd.concat([pd.DataFrame(target), models['pred_combined']], axis=1)\n",
    "        actual_pred.columns = ['actual','pred']\n",
    "        actual_pred = actual_pred.sort_values(by='pred', ascending=0)\n",
    "        top_5_percent_cutoff = int(len(actual_pred)*0.05)\n",
    "        accuracy = sum(actual_pred['actual'][:top_5_percent_cutoff])/top_5_percent_cutoff\n",
    "        accuracy_cup.append([accuracy, str(w)])\n",
    "\n",
    "    accuracy_cup =  pd.DataFrame(accuracy_cup)\n",
    "    accuracy_cup.columns = ['accuracy', 'weights']\n",
    "    max_accuracy = accuracy_cup[accuracy_cup.accuracy == accuracy_cup.accuracy.max()].values[0][0]\n",
    "    best_weights = accuracy_cup[accuracy_cup.accuracy == accuracy_cup.accuracy.max()].values[0][1]\n",
    "    print('random.seed:', random_state, 'n_random_runs:', n_random_runs)\n",
    "    print('Max accuracy: ', max_accuracy)\n",
    "    print('Weights: ', best_weights)\n",
    "    return best_weights, max_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensemble \n",
    "\n",
    "pred_combined = (lr1_pred * 0.23161068   + xgb1_pred * 0.35029858 + \n",
    "                 gbm1_pred * 0.05933682 + rf1_pred * 0.07465767 + ert1_pred * 0.28409625)\n",
    "\n",
    "top_5_accuracy(md_valid_y, pred_combined, 'ensembled_models')\n",
    "print_auc(md_valid_y, pred_combined, 'ensembled_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_5_accuracy(actual, pred, model_name_to_print):\n",
    "    actual_pred = pd.concat([pd.DataFrame(actual), pd.DataFrame(pred)], axis=1)\n",
    "    actual_pred.columns = ['actual','pred']\n",
    "    actual_pred = actual_pred.sort_values(by='pred', ascending=0)\n",
    "    top_5_percent_cutoff = int(len(actual_pred)*0.05)\n",
    "    accuracy = sum(actual_pred['actual'][:top_5_percent_cutoff])/top_5_percent_cutoff\n",
    "    result = str(model_name_to_print) + ' top 5% accuracy: ' + str(np.round(accuracy, 5))\n",
    "    return result\n",
    "\n",
    "def top_5_accuracy(actual, pred, model_name_to_print):\n",
    "    actual_pred = pd.concat([pd.DataFrame(actual), pd.DataFrame(pred)], axis=1)\n",
    "    actual_pred.columns = ['actual','pred']\n",
    "    actual_pred = actual_pred.sort_values(by='pred', ascending=0)\n",
    "    top_5_percent_cutoff = int(len(actual_pred)*0.05)\n",
    "    accuracy = sum(actual_pred['actual'][:top_5_percent_cutoff])/top_5_percent_cutoff\n",
    "    print(str(model_name_to_print), ' top 5% accuracy: ', np.round(accuracy, 5))\n",
    "    return np.round(accuracy, 5)\n",
    "\n",
    "def print_auc(actual, pred, model_name_to_print, plot=False):\n",
    "    fpr, tpr, thresholds = roc_curve(actual, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if plot:\n",
    "        plt.plot(fpr, tpr, label=model_name_to_print)\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(('ROC of ' + model_name_to_print + ', AUC = '+ str(np.round(roc_auc, 5))))\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.show()\n",
    "    print(str(model_name_to_print), ' AUC: ', np.round(roc_auc, 5))\n",
    "    return roc_auc\n",
    "\n",
    "def print_precision_recall_curve(actual, pred, model_name_to_print):\n",
    "    precision, recall, threshold = precision_recall_curve(actual, pred)\n",
    "    average_precision = average_precision_score(actual, pred)\n",
    "    plt.plot(recall, precision, color='b', alpha=1, label=model_name_to_print)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: Average Precision={0:0.4f}'.format(\n",
    "        average_precision))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer(missing_values='NaN', strategy='mean')),\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('rf', RandomForestRegressor()),\n",
    "])\n",
    "\n",
    "# Parameters can be passed anywhere in the pipeline, so they begin with the step name\n",
    "param_grid = {\n",
    "        'rf__n_estimators': [100, 500, 750, 1000],\n",
    "        'rf__max_features': [\"sqrt\", 0.33],\n",
    "        'rf__max_depth': [3, 5, 7, 10],\n",
    "        'rf__min_samples_split': [2, 15, 30, 50]\n",
    "}\n",
    "\n",
    "# Create the CV Grid Search with the pipeline as the model\n",
    "pipeline_grid = GridSearchCV(pipeline, param_grid, cv=kf(), n_jobs=12)\n",
    "pipeline_grid.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "train_score = mean_squared_error(y_train, pipeline_grid.predict(X_train))\n",
    "test_score = mean_squared_error(y_test, pipeline_grid.predict(X_test))\n",
    "\n",
    "print(\"MSE on Train: {}, on Test: {}\".format(train_score, test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
